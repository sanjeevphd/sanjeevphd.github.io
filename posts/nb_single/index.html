<!DOCTYPE html>
<html lang="en-us">
<head>

    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="theme" content="hugo-tachyons">
    <meta name="generator" content="Hugo 0.55.6" />
    <meta name="author" content="smalalur-gh">

    
    <link
href="https://fonts.googleapis.com/css?family=Space+Mono|Cutive+Mono" rel="stylesheet">

    
    <link rel="stylesheet" href="localhost:1313css/normalize.css">
    <link rel="stylesheet" href="localhost:1313css/tachyons.min.css">
    

    
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/github.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    <link rel="shortcut icon" href="localhost:1313img/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="#ZgotmplZ">

    <title>The Naive Bayes Classifier | SM</title>

    
    <style>
        body {
	  font-family: 'Avenir Next';
	  background: #F4F4F4;
	}
	code {
	  font-family: 'Cutive Mono','Space Mono';
	  font-size: 1.25rem;
	  line-height: 1.25;
	}
	blockquote {
	  padding-left: 2rem;
	  border-left-style: solid;
	  border-left-width: 1px;
	  border-width: .25rem;
	  border-color: #AAAAAA;
	  color: #777777;
	}
	h1, h2, h3, h4, h5, h6 {
		font-weight: 500;
	}
    </style>

</head>


    <body class="relative min-vh-100 pb6">
    
    <header class="pv3 pv4-ns tc">

    <a class="link f2 f1-ns black-90 db" href="localhost:1313">Sanjeev Malalur, PhD</a>


</header>


    


<nav class="tc">

<a href="localhost:1313/publications/" class="link light-silver db dib-ns mh3 pa2 f5 fw4 ttu tracked hover-near-black">Publications</a>

<a href="localhost:1313/resume/" class="link light-silver db dib-ns mh3 pa2 f5 fw4 ttu tracked hover-near-black">Resume</a>

<a href="localhost:1313/humanifesto/" class="link light-silver db dib-ns mh3 pa2 f5 fw4 ttu tracked hover-near-black">(hu)manifesto</a>

<a href="localhost:1313/posts/" class="link light-silver db dib-ns mh3 pa2 f5 fw4 ttu tracked hover-near-black">Posts</a>

</nav>


        <article class="mw7 center pa3 pa4-ns">
	    <h1 class="f1 f-3-ns fw5 mt0 mb2 pb2 black-90">The Naive Bayes Classifier</h1>
	    <div class="db gray">
	      <time class="dib pr3 ttu">01 Jan 0001</time>
	      <p class="dib pr3 ttu">1919 words</p>
	      <p class="dib pr3">(~10 min read)</p> 
	    </div>
	    <div class="f5 bb lh-title black-90">
	    <div class="document" id="not-so-naive-after-all">
<h1 class="title">...not so naive after all</h1>

<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#introduction" id="id1">Introduction</a></li>
<li><a class="reference internal" href="#theory" id="id2">Theory</a><ul>
<li><a class="reference internal" href="#classifier-modes" id="id3">Classifier Modes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#types-of-naive-bayes" id="id4">Types of Naive Bayes</a><ul>
<li><a class="reference internal" href="#continuous-random-variables" id="id5">Continuous Random Variables</a><ul>
<li><a class="reference internal" href="#gaussian-naive-bayes-from-scratch" id="id6">Gaussian Naive Bayes from Scratch</a></li>
<li><a class="reference internal" href="#naive-bayes-decision-boundary" id="id7">Naive Bayes Decision Boundary</a></li>
<li><a class="reference internal" href="#flexible-naive-bayes" id="id8">Flexible Naive Bayes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#discrete-random-variables" id="id9">Discrete Random Variables</a><ul>
<li><a class="reference internal" href="#multinomial-naive-bayes" id="id10">Multinomial Naive Bayes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#naive-insights" id="id11">Naive Insights</a><ul>
<li><a class="reference internal" href="#power-despite-naivete" id="id12">Power Despite Naivete</a></li>
<li><a class="reference internal" href="#pros-and-cons" id="id13">Pros and Cons</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references" id="id14">References</a><ul>
<li><a class="reference internal" href="#book-chapters" id="id15">Book Chapters</a></li>
<li><a class="reference internal" href="#implementations-python-specific" id="id16">Implementations (Python specific):</a></li>
<li><a class="reference internal" href="#online-tutorial-posts" id="id17">Online Tutorial &amp; Posts</a></li>
<li><a class="reference internal" href="#additional-resources" id="id18">Additional Resources</a></li>
<li><a class="reference internal" href="#spam-filtering" id="id19">Spam Filtering</a></li>
</ul>
</li>
</ul>
</div>
<!--  -->
<p><em>Updated</em>:</p>
<p><strong>Summary</strong>: An indepth look at the classic Naive Bayes classifier with
code examples, applications and more!</p>
<div class="admonition admonition-todo">
<p class="first admonition-title">TODO</p>
<p class="last">Figure out how to add Read More... link here so Tumblr only shows the
summary for each post</p>
</div>
<div class="contents docutils container">
Sections</div>
<hr class="docutils" />
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id1">Introduction</a></h2>
<p>Naive Bayes, a classical model for decision making with roots in
probability theory has been around for a long time and is both revered
and used as a punching bag by more modern methods. While it is
prominently featured in machine learning texts and libraries, there is
not so much <em>learning</em> happening with the Naive Bayes, if one takes the
view that learning happens via iteration and adjustment of some internal
parameters. Nope. As we will see, the Naive Bayes, with simple (yet
questionable) assumptions about the data looks at the data and makes a
decision based on the <em>likelihood</em> and the <em>prior</em> distributions.
Nothing to iterate, nothing to tune (well, not really, but more on that
later). In fact, for the most part, it operates from a <em>generative</em> or
<em>sampling</em> paradigm. Although it is interesting to note that it can also
act as a typical classifier (say, like the logistic regression model)
and operate out of a <em>discriminative</em> paradigm (more below).</p>
<p>So how does it all fit? Well, machine learning problems, specifically
the supervised learning problem, can be posed as a general problem of
learning a function <span class="formula"><i>f</i> : <i>X</i> −  &gt; <i>Y</i></span> that maps the inupt X to the
outputs Y as best as possible. Supervised classification problems
involve Y being discrete valued, taking on one of C classes or
categories. Now, from probability theory, finding the learning function
is equivalent to finding the posterior probability <span class="formula"><i>P</i>(<i>Y</i>|<i>X</i>)</span>.</p>
<blockquote>
<p>Q: &quot;Ok, so how to estimate <span class="formula"><i>P</i>(<i>Y</i>|<i>X</i>)</span>?&quot;</p>
<p>A: (in a deep voice) &quot;<strong>The Bayes Theorem</strong>&quot;</p>
</blockquote>
<div class="admonition admonition-todo">
<p class="first admonition-title">TODO</p>
<p class="last">Find the LateX equivalent for &quot;-&gt;&quot; symbol and replace it in the
sentence above.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="theory">
<h2><a class="toc-backref" href="#id2">Theory</a></h2>
<p>Broadly the idea is to view the inputs and outputs as a joint
distribution described by <span class="formula"><i>P</i>(<b>x</b>, <i>i</i><sub><i>c</i></sub>)</span>. The joint
distribution can be expressed in two ways.</p>
<div class="formula">
<i>P</i>(<b>x</b>, <i>i</i><sub><i>c</i></sub>) = <i>p</i>(<b>x</b>|<i>i</i><sub><i>c</i></sub>)⋅<i>P</i>(<i>i</i><sub><i>c</i></sub>) = <i>p</i>(<i>i</i><sub><i>c</i></sub>|<b>x</b>)⋅<i>P</i>(<b>x</b>)
</div>
<div class="admonition admonition-todo">
<p class="first admonition-title">TODO</p>
<p class="last">Explain the notation used</p>
</div>
<p>Rearranging the terms, we get the Bayes rule as</p>
<div class="formula">
<i>p</i>(<i>i</i><sub><i>c</i></sub>|<b>x</b>) = <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>p</i>(<b>x</b>|<i>i</i><sub><i>c</i></sub>)⋅<i>P</i>(<i>i</i><sub><i>c</i></sub>)</span><span class="ignored">)/(</span><span class="denominator"><i>P</i>(<b>x</b>)</span><span class="ignored">)</span></span>
</div>
<p>This is a way of expressing the posterior probability using the
likelihood, priors and evidence, i.e.,</p>
<div class="formula">
<i>posterior</i> = <span class="fraction"><span class="ignored">(</span><span class="numerator"><i>likelihood</i> × <i>prior</i></span><span class="ignored">)/(</span><span class="denominator"><i>evidence</i></span><span class="ignored">)</span></span>
</div>
<p>Note that the denominator <em>evidence</em> is merely a scaling factor to
ensure that the <em>posterior</em> is within the range <span class="formula">(0, 1)</span>, like all
probabilities should. The likelihood and priors form the most impact on
the estimated posterior probability.</p>
<p>So far so good, but...</p>
<p>Applying the Bayes rule, in its present form, to classification problems
is impractical. Why? Consider the case of boolean variables where both Y
and X take on either 0 or 1 for values. If each X in the data is an
n-dimensional vector, then we will need to estimate about <span class="formula">2<sup><i>n</i></sup></span>
parameters, which can explode for even modest values of n. Also, what
about multi-valued or continuous valued variables, with
interactions....aaargh!</p>
<p>(Deep breaths...)</p>
<blockquote>
<em>Keep Calm and Naive Bayes on!</em></blockquote>
<p>It must be noted that there is an entire field called <em>Bayesian
Learning</em> dedicated to addressing this problem.</p>
<p>The Naive Bayes (NB) glosses over some of the issues by simply assuming
that the <span class="formula"><b>x</b></span> are <em>independent</em>, for (or within) each
class. Under this assumption, the conditional probability (likelihood)
can be expressed as:</p>
<div class="formula">
<i>p</i>(<b>x</b>|<i>i</i><sub><i>c</i></sub>) = <span class="limits"><sup class="limit"><i>P</i></sup><span class="limit">∏</span><sub class="limit"><i>j</i> = 1</sub></span><i>p</i>(<i>x</i><sub><i>j</i></sub>|<i>i</i><sub><i>c</i></sub>)
</div>
<div class="admonition admonition-todo">
<p class="first admonition-title">TODO</p>
<p class="last">Explain how under independence, the joint probability becomes a
product form of same terms and hence results in the form above.</p>
</div>
<p>Depending on the assumption on the underlying likelihood distribution
(ex. Gaussian, Binomial, Poisson), different NB classifiers can be
generated for different tasks.</p>
<div class="section" id="classifier-modes">
<h3><a class="toc-backref" href="#id3">Classifier Modes</a></h3>
<p>A typical classifier operates in one of two <em>modes</em> or <em>paradigm</em>.</p>
<ul class="simple">
<li>sampling (generative) paradigm, which focuses on the individual
distributions of the classes, comparing these to indirectly produce a
comparison between the classes.</li>
<li>diagnostic (discriminative) paradigm, which focuses on the
differences between the classes, i.e., on discriminating them</li>
</ul>
<p>The nice thing about NB is that it can be viewed from both perspectives.</p>
<p>A quick example, using the <a class="reference external" href="https://scikit-learn.org">Scikit-Learn</a>
Python library, illustrates the typical workflow involved intraining a
Naive Bayes classifier and testing its performance on test data (i.e.,
data not used or seen by the classifier during training).</p>
<pre class="code python literal-block">
<span class="literal string doc">&quot;&quot;&quot;Example of using Gaussing Naive Bayes for Classification

Ref: https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes

&quot;&quot;&quot;</span>

<span class="keyword namespace">from</span> <span class="name namespace">sklearn.datasets</span> <span class="keyword namespace">import</span> <span class="name">load_iris</span>
<span class="keyword namespace">from</span> <span class="name namespace">sklearn.model_selection</span> <span class="keyword namespace">import</span> <span class="name">train_test_split</span>
<span class="keyword namespace">from</span> <span class="name namespace">sklearn.naive_bayes</span> <span class="keyword namespace">import</span> <span class="name">GaussianNB</span>

<span class="name">X</span><span class="punctuation">,</span> <span class="name">y</span> <span class="operator">=</span> <span class="name">load_iris</span><span class="punctuation">(</span><span class="name">return_X_y</span><span class="operator">=</span><span class="keyword constant">True</span><span class="punctuation">)</span>
<span class="name">X_train</span><span class="punctuation">,</span> <span class="name">X_test</span><span class="punctuation">,</span> <span class="name">y_train</span><span class="punctuation">,</span> <span class="name">y_test</span> <span class="operator">=</span> <span class="name">train_test_split</span><span class="punctuation">(</span><span class="name">X</span><span class="punctuation">,</span> <span class="name">y</span><span class="punctuation">,</span> <span class="name">test_size</span><span class="operator">=</span><span class="literal number float">0.5</span><span class="punctuation">,</span> <span class="name">random_state</span><span class="operator">=</span><span class="literal number integer">0</span><span class="punctuation">)</span>

<span class="name">gnb_clf</span> <span class="operator">=</span> <span class="name">GaussianNB</span><span class="punctuation">()</span>
<span class="name">gnb_clf</span><span class="operator">.</span><span class="name">fit</span><span class="punctuation">(</span><span class="name">X_train</span><span class="punctuation">,</span> <span class="name">y_train</span><span class="punctuation">)</span>
<span class="name">y_pred</span> <span class="operator">=</span> <span class="name">gnb_clf</span><span class="operator">.</span><span class="name">predict</span><span class="punctuation">(</span><span class="name">X_test</span><span class="punctuation">)</span>
<span class="name builtin">print</span><span class="punctuation">(</span><span class="literal string affix">f</span><span class="literal string double">&quot;Total number of samples in testing set: </span><span class="literal string interpol">{</span><span class="name">X_test</span><span class="operator">.</span><span class="name">shape</span><span class="punctuation">[</span><span class="literal number integer">0</span><span class="punctuation">]</span><span class="literal string interpol">}</span><span class="literal string double">&quot;</span><span class="punctuation">)</span>
<span class="name builtin">print</span><span class="punctuation">(</span><span class="literal string affix">f</span><span class="literal string double">&quot;Number of mislabeled points in the test set: </span><span class="literal string interpol">{</span><span class="punctuation">(</span><span class="name">y_test</span> <span class="operator">!=</span> <span class="name">y_pred</span><span class="punctuation">)</span><span class="operator">.</span><span class="name">sum</span><span class="punctuation">()</span><span class="literal string interpol">}</span><span class="literal string double">&quot;</span><span class="punctuation">)</span>
</pre>
<pre class="code shell literal-block">
Total number of samples in testing set: <span class="literal number">75</span>
Number of mislabeled points in the <span class="name builtin">test</span> set: <span class="literal number">4</span>
</pre>
<p>While this <em>off the shelf</em> approach is fine for quick comparison or
obtaining a baseilne, implementing from scratch will lead to a deeper
understanding on the inner workings of the Naive Bayes classifier.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="types-of-naive-bayes">
<h2><a class="toc-backref" href="#id4">Types of Naive Bayes</a></h2>
<p>Applications in text analytics - document categorization, sentiment
analysis, spam identification</p>
<p>Apply NB to datasets like Titanic or MNIST or other popular ones.</p>
<p>Can multiple distributions be used for different subsets of features and
be combined to form a joint discriminating function?</p>
<div class="section" id="continuous-random-variables">
<h3><a class="toc-backref" href="#id5">Continuous Random Variables</a></h3>
<div class="section" id="gaussian-naive-bayes-from-scratch">
<h4><a class="toc-backref" href="#id6">Gaussian Naive Bayes from Scratch</a></h4>
<p>Below is an implementation of a Gaussian Naive Bayes classifier from
scratch.</p>
<pre class="code python literal-block">
<span class="literal string doc">&quot;&quot;&quot;Module for implementing Naive Bayes algorithm for classification.

These algorithms assume strong independence of the features within a class and
the type of the likelihood function (ex. Gaussian, Bernoulli, Multinomial,
etc.)
&quot;&quot;&quot;</span>

<span class="keyword namespace">from</span> <span class="name namespace">collections</span> <span class="keyword namespace">import</span> <span class="name">Counter</span>

<span class="keyword namespace">import</span> <span class="name namespace">numpy</span> <span class="keyword">as</span> <span class="name namespace">np</span>

<span class="keyword">class</span> <span class="name class">NaiveGaussian</span><span class="punctuation">():</span>
    <span class="literal string doc">&quot;&quot;&quot;Naive Bayes classifier assuming Gaussian likelihood function

    For more details see :ref:`Naive Bayes &lt;../docs/nb.html&gt;`.

    Attributes
    ----------

    class_count_: array, shape (nb_classes,)
        Number of samples per class
    class_prior_: array, shape (nb_classes,)
        Probability of occurrance of each class
    class_labels_: array, shape (nb_classes,)
        Class labels or IDs present in the data
    feature_mean_: array, shape (nb_classes, nb_features)
        Input feature mean per class
    feature_variance_: array, shape (nb_classes, nb_features)
        Input feature variance per class

    Example Usage
    -------------

    &gt;&gt;&gt; import numpy as np
    &gt;&gt;&gt; np.random.seed(0)
    &gt;&gt;&gt; X = np.random.randn(100,2)
    &gt;&gt;&gt; y = X.sum(axis=1) &gt; 0
    &gt;&gt;&gt; clf = NaiveGaussian().fit(X, y)
    &gt;&gt;&gt; Xtest = np.random.randn(20, 2)
    &gt;&gt;&gt; ytest = Xtest.sum(axis=1) &gt; 0
    &gt;&gt;&gt; ypred = clf.predict(Xtest)
    &gt;&gt;&gt; err = np.sum(ytest != ypred)
    &gt;&gt;&gt; print(f'Misclassified {err} out of {ytest.shape[0]} samples')

    &quot;&quot;&quot;</span>

    <span class="keyword">def</span> <span class="name function">fit</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="punctuation">,</span> <span class="name">X</span><span class="punctuation">,</span> <span class="name">y</span><span class="punctuation">):</span>
        <span class="literal string doc">&quot;&quot;&quot;Fit a Naive Bayes classifier assuming Gaussian likelihood

        Parameters
        ----------

        X: array-like, shape (nb_samples, nb_features)
           Training data
        y: array-like, shape (nb_samples)
           Target output labels or classes

        Returns
        -------

        self: object
        &quot;&quot;&quot;</span>

        <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">nb_features_</span> <span class="operator">=</span> <span class="name">X</span><span class="operator">.</span><span class="name">shape</span><span class="punctuation">[</span><span class="literal number integer">1</span><span class="punctuation">]</span>
        <span class="comment single"># compute class labels, counts and priors</span>
        <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_counts_</span> <span class="operator">=</span> <span class="name">Counter</span><span class="punctuation">(</span><span class="name">y</span><span class="punctuation">)</span>
        <span class="comment single"># TODO does sorting the keys matter?</span>
        <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_labels_</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">asarray</span><span class="punctuation">(</span><span class="name builtin">sorted</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_counts_</span><span class="operator">.</span><span class="name">keys</span><span class="punctuation">()))</span>
        <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">nb_classes_</span> <span class="operator">=</span> <span class="name builtin">len</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_labels_</span><span class="punctuation">)</span>
        <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_prior_</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">asarray</span><span class="punctuation">([</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_counts_</span><span class="punctuation">[</span><span class="name">k</span><span class="punctuation">]</span><span class="operator">/</span><span class="name">y</span><span class="operator">.</span><span class="name">shape</span><span class="punctuation">[</span><span class="literal number integer">0</span><span class="punctuation">]</span>
            <span class="keyword">for</span> <span class="name">k</span> <span class="operator word">in</span> <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_labels_</span><span class="punctuation">])</span>
        <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">feature_mean_</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">zeros</span><span class="punctuation">((</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">nb_classes_</span><span class="punctuation">,</span> <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">nb_features_</span><span class="punctuation">))</span>
        <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">feature_variance_</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">zeros</span><span class="punctuation">((</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">nb_classes_</span><span class="punctuation">,</span>
            <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">nb_features_</span><span class="punctuation">))</span>
        <span class="keyword">for</span> <span class="name">c</span> <span class="operator word">in</span> <span class="name builtin">range</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">nb_classes_</span><span class="punctuation">):</span>
            <span class="name">Xc</span> <span class="operator">=</span> <span class="name">X</span><span class="punctuation">[</span><span class="name">y</span> <span class="operator">==</span> <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_labels_</span><span class="punctuation">[</span><span class="name">c</span><span class="punctuation">]]</span>
            <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">feature_mean_</span><span class="punctuation">[</span><span class="name">c</span><span class="punctuation">,</span> <span class="punctuation">:]</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">mean</span><span class="punctuation">(</span><span class="name">Xc</span><span class="punctuation">,</span> <span class="name">axis</span><span class="operator">=</span><span class="literal number integer">0</span><span class="punctuation">)</span>
            <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">feature_variance_</span><span class="punctuation">[</span><span class="name">c</span><span class="punctuation">,</span> <span class="punctuation">:]</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">var</span><span class="punctuation">(</span><span class="name">Xc</span><span class="punctuation">,</span> <span class="name">axis</span><span class="operator">=</span><span class="literal number integer">0</span><span class="punctuation">)</span>

        <span class="keyword">return</span> <span class="name builtin pseudo">self</span>

    <span class="keyword">def</span> <span class="name function">_joint_log_likelihood</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="punctuation">,</span> <span class="name">X</span><span class="punctuation">):</span>
        <span class="literal string doc">&quot;&quot;&quot;Return the Joint Loglikelihood value&quot;&quot;&quot;</span>
        <span class="name">joint_log_likelihood</span> <span class="operator">=</span> <span class="punctuation">[]</span>
        <span class="keyword">for</span> <span class="name">i</span> <span class="operator word">in</span> <span class="name builtin">range</span><span class="punctuation">(</span><span class="name">np</span><span class="operator">.</span><span class="name">size</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_labels_</span><span class="punctuation">)):</span>
            <span class="name">jointi</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">log</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_prior_</span><span class="punctuation">[</span><span class="name">i</span><span class="punctuation">])</span>
            <span class="name">n_ij</span> <span class="operator">=</span> <span class="operator">-</span> <span class="literal number float">0.5</span> <span class="operator">*</span> <span class="name">np</span><span class="operator">.</span><span class="name">sum</span><span class="punctuation">(</span><span class="name">np</span><span class="operator">.</span><span class="name">log</span><span class="punctuation">(</span><span class="literal number float">2.</span> <span class="operator">*</span> <span class="name">np</span><span class="operator">.</span><span class="name">pi</span> <span class="operator">*</span>
                <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">feature_variance_</span><span class="punctuation">[</span><span class="name">i</span><span class="punctuation">,</span> <span class="punctuation">:]))</span>
            <span class="name">n_ij</span> <span class="operator">-=</span> <span class="literal number float">0.5</span> <span class="operator">*</span> <span class="name">np</span><span class="operator">.</span><span class="name">sum</span><span class="punctuation">(((</span><span class="name">X</span> <span class="operator">-</span> <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">feature_mean_</span><span class="punctuation">[</span><span class="name">i</span><span class="punctuation">,</span> <span class="punctuation">:])</span> <span class="operator">**</span> <span class="literal number integer">2</span><span class="punctuation">)</span> <span class="operator">/</span>
                                <span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">feature_variance_</span><span class="punctuation">[</span><span class="name">i</span><span class="punctuation">,</span> <span class="punctuation">:]),</span> <span class="literal number integer">1</span><span class="punctuation">)</span>
            <span class="name">joint_log_likelihood</span><span class="operator">.</span><span class="name">append</span><span class="punctuation">(</span><span class="name">jointi</span> <span class="operator">+</span> <span class="name">n_ij</span><span class="punctuation">)</span>

        <span class="name">joint_log_likelihood</span> <span class="operator">=</span> <span class="name">np</span><span class="operator">.</span><span class="name">array</span><span class="punctuation">(</span><span class="name">joint_log_likelihood</span><span class="punctuation">)</span><span class="operator">.</span><span class="name">T</span>

        <span class="keyword">return</span> <span class="name">joint_log_likelihood</span>

    <span class="keyword">def</span> <span class="name function">predict</span><span class="punctuation">(</span><span class="name builtin pseudo">self</span><span class="punctuation">,</span> <span class="name">X</span><span class="punctuation">):</span>
        <span class="literal string doc">&quot;&quot;&quot;
        Perform classification on an array of test vectors X.

        Parameters
        ----------

        X : array-like of shape (n_samples, n_features)

        Returns
        -------

        C : ndarray of shape (n_samples,)
            Predicted target values for X

        &quot;&quot;&quot;</span>
        <span class="name">jll</span> <span class="operator">=</span> <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">_joint_log_likelihood</span><span class="punctuation">(</span><span class="name">X</span><span class="punctuation">)</span>

        <span class="keyword">return</span> <span class="name builtin pseudo">self</span><span class="operator">.</span><span class="name">class_labels_</span><span class="punctuation">[</span><span class="name">np</span><span class="operator">.</span><span class="name">argmax</span><span class="punctuation">(</span><span class="name">jll</span><span class="punctuation">,</span> <span class="name">axis</span><span class="operator">=</span><span class="literal number integer">1</span><span class="punctuation">)]</span>
</pre>
</div>
<div class="section" id="naive-bayes-decision-boundary">
<h4><a class="toc-backref" href="#id7">Naive Bayes Decision Boundary</a></h4>
<p>Assumes the likelihood to be a Gaussian distribution.</p>
<pre class="code python literal-block">
<span class="keyword namespace">import</span> <span class="name namespace">numpy</span>
<span class="keyword namespace">import</span> <span class="name namespace">matplotlib.pyplot</span> <span class="keyword">as</span> <span class="name namespace">plt</span>
<span class="keyword namespace">import</span> <span class="name namespace">seaborn</span> <span class="keyword">as</span> <span class="name namespace">sns</span>
<span class="name">sns</span><span class="operator">.</span><span class="name">set</span><span class="punctuation">()</span>
<span class="keyword namespace">from</span> <span class="name namespace">sklearn.naive_bayes</span> <span class="keyword namespace">import</span> <span class="name">GaussianNB</span>

<span class="comment single"># generate bivariate random variables for 2 classes</span>
<span class="name">seed</span> <span class="operator">=</span> <span class="literal number integer">0</span>
<span class="name">numpy</span><span class="operator">.</span><span class="name">random</span><span class="operator">.</span><span class="name">RandomState</span><span class="punctuation">(</span><span class="name">seed</span><span class="punctuation">)</span>
<span class="name">mean</span> <span class="operator">=</span> <span class="punctuation">[</span><span class="literal number integer">0</span><span class="punctuation">,</span> <span class="literal number integer">0</span><span class="punctuation">]</span>
<span class="name">cov</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">identity</span><span class="punctuation">(</span><span class="literal number integer">2</span><span class="punctuation">)</span>
<span class="name">x1</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">random</span><span class="operator">.</span><span class="name">multivariate_normal</span><span class="punctuation">(</span><span class="name">mean</span><span class="punctuation">,</span> <span class="name">cov</span><span class="punctuation">,</span> <span class="name">size</span><span class="operator">=</span><span class="punctuation">(</span><span class="literal number integer">2</span><span class="punctuation">,</span> <span class="literal number integer">100</span><span class="punctuation">))</span>
<span class="name">y1</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">zeros</span><span class="punctuation">(</span><span class="name">x1</span><span class="operator">.</span><span class="name">shape</span><span class="punctuation">[</span><span class="literal number integer">1</span><span class="punctuation">])</span>
<span class="name">x2</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">random</span><span class="operator">.</span><span class="name">multivariate_normal</span><span class="punctuation">([</span><span class="literal number integer">0</span><span class="punctuation">,</span> <span class="literal number integer">2</span><span class="punctuation">],</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">diag</span><span class="punctuation">([</span><span class="literal number integer">1</span><span class="punctuation">,</span> <span class="literal number integer">2</span><span class="punctuation">]),</span> <span class="name">size</span><span class="operator">=</span><span class="punctuation">(</span><span class="literal number integer">2</span><span class="punctuation">,</span> <span class="literal number integer">100</span><span class="punctuation">))</span>
<span class="name">y2</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">ones</span><span class="punctuation">(</span><span class="name">x2</span><span class="operator">.</span><span class="name">shape</span><span class="punctuation">[</span><span class="literal number integer">1</span><span class="punctuation">])</span>

<span class="comment single"># concat data into a single set</span>
<span class="name">X</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">vstack</span><span class="punctuation">((</span><span class="name">x1</span><span class="punctuation">[</span><span class="literal number integer">0</span><span class="punctuation">],</span> <span class="name">x2</span><span class="punctuation">[</span><span class="literal number integer">0</span><span class="punctuation">]))</span>
<span class="name">y</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">hstack</span><span class="punctuation">((</span><span class="name">y1</span><span class="punctuation">,</span> <span class="name">y2</span><span class="punctuation">))</span>

<span class="comment single"># train a Gaussian NB classifier</span>
<span class="name">clf</span> <span class="operator">=</span> <span class="name">GaussianNB</span><span class="punctuation">()</span><span class="operator">.</span><span class="name">fit</span><span class="punctuation">(</span><span class="name">X</span><span class="punctuation">,</span> <span class="name">y</span><span class="punctuation">)</span>

<span class="comment single"># test data</span>
<span class="name">test_samples</span> <span class="operator">=</span> <span class="literal number integer">5000</span>
<span class="name">Xtest</span> <span class="operator">=</span> <span class="name">numpy</span><span class="operator">.</span><span class="name">random</span><span class="operator">.</span><span class="name">uniform</span><span class="punctuation">([</span><span class="operator">-</span><span class="literal number integer">3</span><span class="punctuation">,</span> <span class="operator">-</span><span class="literal number integer">2</span><span class="punctuation">],</span> <span class="punctuation">[</span><span class="literal number integer">4</span><span class="punctuation">,</span> <span class="literal number integer">6</span><span class="punctuation">],</span> <span class="name">size</span><span class="operator">=</span><span class="punctuation">(</span><span class="name">test_samples</span><span class="punctuation">,</span> <span class="literal number integer">2</span><span class="punctuation">))</span>
<span class="name">ypred</span> <span class="operator">=</span> <span class="name">clf</span><span class="operator">.</span><span class="name">predict</span><span class="punctuation">(</span><span class="name">Xtest</span><span class="punctuation">)</span> <span class="comment single"># predictions</span>

<span class="comment single"># plots</span>
<span class="name">fig</span><span class="punctuation">,</span> <span class="name">ax</span> <span class="operator">=</span> <span class="name">plt</span><span class="operator">.</span><span class="name">subplots</span><span class="punctuation">()</span>
<span class="name">ax</span><span class="operator">.</span><span class="name">scatter</span><span class="punctuation">(</span><span class="name">X</span><span class="punctuation">[:,</span> <span class="literal number integer">0</span><span class="punctuation">],</span> <span class="name">X</span><span class="punctuation">[:,</span> <span class="literal number integer">1</span><span class="punctuation">],</span> <span class="name">c</span><span class="operator">=</span><span class="name">y</span><span class="punctuation">,</span> <span class="name">cmap</span><span class="operator">=</span><span class="literal string single">'RdBu'</span><span class="punctuation">,</span> <span class="name">s</span><span class="operator">=</span><span class="literal number integer">50</span><span class="punctuation">)</span>
<span class="name">ax</span><span class="operator">.</span><span class="name">scatter</span><span class="punctuation">(</span><span class="name">Xtest</span><span class="punctuation">[:,</span> <span class="literal number integer">0</span><span class="punctuation">],</span> <span class="name">Xtest</span><span class="punctuation">[:,</span> <span class="literal number integer">1</span><span class="punctuation">],</span> <span class="name">c</span><span class="operator">=</span><span class="name">ypred</span><span class="punctuation">,</span> <span class="name">alpha</span><span class="operator">=</span><span class="literal number float">0.1</span><span class="punctuation">,</span> <span class="name">s</span><span class="operator">=</span><span class="literal number integer">10</span><span class="punctuation">,</span> <span class="name">cmap</span><span class="operator">=</span><span class="literal string single">'RdBu'</span><span class="punctuation">)</span>
<span class="name">plt</span><span class="operator">.</span><span class="name">show</span><span class="punctuation">()</span>
<span class="name">plt</span><span class="operator">.</span><span class="name">savefig</span><span class="punctuation">(</span><span class="literal string single">'bivariate_gauss_nb_boundary.png'</span><span class="punctuation">,</span> <span class="name">dpi</span><span class="operator">=</span><span class="literal number integer">200</span><span class="punctuation">)</span>
</pre>
<div class="align-center figure">
<img alt="Decision boundary for a bivariate Gaussian Naive Bayes classifier" src="../tutorials/bivariate_gauss_nb_boundary.png" style="width: 50.0%;" />
<p class="caption">Decision boundary for a bivariate Gaussian Naive Bayes classifier</p>
</div>
<p>Under the hood</p>
<p>fit() computes and stores the mean and deviations of all input features,
per class along with the class priors, number of classes, class IDs,
etc.</p>
<p>predict() computes the joing log-likelihood function and assigns the
class to the one with the maximum value. Include mathematical
formulation for Gaussian case.</p>
</div>
<div class="section" id="flexible-naive-bayes">
<h4><a class="toc-backref" href="#id8">Flexible Naive Bayes</a></h4>
</div>
</div>
<div class="section" id="discrete-random-variables">
<h3><a class="toc-backref" href="#id9">Discrete Random Variables</a></h3>
<ul class="simple">
<li>Bernoulli</li>
<li>Binomial</li>
<li>Multinomial</li>
<li>Multinomial with Binary features</li>
</ul>
<div class="section" id="multinomial-naive-bayes">
<h4><a class="toc-backref" href="#id10">Multinomial Naive Bayes</a></h4>
<p>Assumes the likelihood to be a multinomial distribution.</p>
<p>Describe the binomial and multinomial distribution and derivation of the
discriminating function.</p>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="naive-insights">
<h2><a class="toc-backref" href="#id11">Naive Insights</a></h2>
<div class="section" id="power-despite-naivete">
<h3><a class="toc-backref" href="#id12">Power Despite Naivete</a></h3>
<p>So why does the NB perform so well? A few reasons.</p>
<ul class="simple">
<li>In most applications, only the decision surface matters</li>
<li>NB can produce complex, nonlinear decision boundaries and can hence
generate elaborate fits</li>
<li>Feature engineering and related variable selection methods applied to
the data beforehand can make the independence assumption not too
detrimental</li>
<li>Complexity of n-univariate likelihood distributions is far lower than
a single n-variate multivariate distribution</li>
</ul>
</div>
<div class="section" id="pros-and-cons">
<h3><a class="toc-backref" href="#id13">Pros and Cons</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="71%" />
<col width="29%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Pros</th>
<th class="head">Cons</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Fast, intuitive, easy to build, Non-iterative</td>
<td>Independence assumption is not practical</td>
</tr>
<tr><td>Does surprisingly well despite assumptions</td>
<td>See what I did there? ;-)</td>
</tr>
<tr><td>Useful in higher dimensions where the independence assumption is more likely to hold</td>
<td>&nbsp;</td>
</tr>
<tr><td>Interpretable - the weights of evidence reveals individual feature contribution to the prediction</td>
<td>&nbsp;</td>
</tr>
<tr><td>Can create nonlinear decision boundaries &amp; complex models</td>
<td>&nbsp;</td>
</tr>
<tr><td>Very few tunable parameters</td>
<td>Very few tunable parameters :-/</td>
</tr>
</tbody>
</table>
<p>Despite the cons, NB is a quick way to get a baseline for comparison
with and improving other models.</p>
<p><strong>Note on Bias-Variance Trade-off for NB</strong></p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="references">
<h2><a class="toc-backref" href="#id14">References</a></h2>
<div class="section" id="book-chapters">
<h3><a class="toc-backref" href="#id15">Book Chapters</a></h3>
<ul class="simple">
<li>Chapter 2 from Richard O. Duda, Peter E. Hart, and David G. Stork.
2000. <em>Pattern Classification</em> (2nd Edition). Wiley-Interscience,
USA.</li>
<li>Chapter 1 from Christopher M. Bishop. 2006. <em>Pattern Recognition and
Machine Learning</em> (Information Science and Statistics).
Springer-Verlag, Berlin, Heidelberg.</li>
<li>Chapter 9 from Xindong Wu and Vipin Kumar. 2009. <em>The Top Ten
Algorithms in Data Mining (1st. ed.)</em>. Chapman &amp; Hall/CRC.</li>
<li>Ch. 3 of Tom Mitchell's book on ML - Generative vs. Discriminant
Classifiers: NB and Logistic Regression</li>
<li>Introduction to Information Retrieval - Ch. 13</li>
<li>NLTK With Python <a class="reference external" href="http://www.nltk.org/book/">online</a>.</li>
<li>Ch. 4 NB and Sentiment Analysis from Speech and Language Processing
text</li>
</ul>
</div>
<div class="section" id="implementations-python-specific">
<h3><a class="toc-backref" href="#id16">Implementations (Python specific):</a></h3>
<ul class="simple">
<li>Scikit-Learn <a class="reference external" href="https://scikit-learn.org/stable/modules/naive_bayes.html">Naive
Bayes</a></li>
</ul>
</div>
<div class="section" id="online-tutorial-posts">
<h3><a class="toc-backref" href="#id17">Online Tutorial &amp; Posts</a></h3>
<ul class="simple">
<li>DONE 2020-04-16 <a class="reference external" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html">In Depth: Naive Bayes
Classification</a>,
Python Data Science Handbook, Jake VanderPlas</li>
<li>DONE 2020-04-16 Scikit-Learn Tutorial on <a class="reference external" href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">Working with Text
Data</a>
(contains skeleton code for exercises)</li>
<li>Sebastian Raschka on <a class="reference external" href="https://sebastianraschka.com/Articles/2014_naive_bayes_1.html">Naive Bayes and Text
Classification</a></li>
<li>Will Kurt on <a class="reference external" href="https://www.countbayesie.com/blog/2019/6/12/logistic-regression-from-bayes-theorem">Logistic Regression and Bayes
Theorem</a>.
This site also contains other interesting posts on probability theory
and related concepts</li>
<li><a class="reference external" href="https://www.python-course.eu/naive_bayes_classifier_introduction.php">Naive Bayes
Classifier</a>
on Python-Course.eu site, implementation from scratch</li>
</ul>
</div>
<div class="section" id="additional-resources">
<h3><a class="toc-backref" href="#id18">Additional Resources</a></h3>
<ul class="simple">
<li>Tutorial <a class="reference external" href="https://www.socher.org/index.php?n=DeepLearningTutorial.DeepLearningTutorial">Deep Learning for NLP (without
magic)</a></li>
<li>Fast.ai Course on <a class="reference external" href="https://github.com/fastai/course-nlp">Natural Language
Processing</a></li>
<li>Stanford Course <a class="reference external" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html">NLP with Deep
Learning</a></li>
<li>Stanford Course (undergrad level) - <a class="reference external" href="https://web.stanford.edu/class/cs124/">From Language to
Information</a></li>
<li>Christopher D. Manning's Courses on Natural Language Processing
<a class="reference external" href="https://nlp.stanford.edu/manning/">listed here</a></li>
<li>Google <a class="reference external" href="https://books.google.com/ngrams">Ngram Viewer</a></li>
</ul>
</div>
<div class="section" id="spam-filtering">
<h3><a class="toc-backref" href="#id19">Spam Filtering</a></h3>
<p>Paul Graham <a class="reference external" href="http://www.paulgraham.com/spam.html">A Plan for Spam</a></p>
</div>
</div>
</div>
	    </div>
            <div class="pv3">
            
	      <nav class="">
	       <a class="f5 f3-ns fw5 db link dim mid-gray dib" href="localhost:1313/tags/">Tags: </a> 
              
                <a class="f5 f4-ns mr2 link white-90 dim dib pa1 br-pill ba bg-black-90" href="localhost:1313/tags/probability">Probability</a> 
              
                <a class="f5 f4-ns mr2 link white-90 dim dib pa1 br-pill ba bg-black-90" href="localhost:1313/tags/naive-bayes">Naive Bayes</a> 
              
                <a class="f5 f4-ns mr2 link white-90 dim dib pa1 br-pill ba bg-black-90" href="localhost:1313/tags/machine-learning">Machine Learning</a> 
              
              </nav>
	    </div>
	</article>
    <script src="localhost:1313/js/fitter-happier-text.js"></script>
  <script>
    var nodes = document.querySelectorAll('[data-fitter-happier-text]');
    fitterHappierText(nodes, { baseline: 16 });
  </script>
  <script type="text/javascript">
    WebFontConfig = { google: { families: [ 'Source+Code+Pro::latin' ] } };
    (function() {
      var wf = document.createElement('script');
      wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
      wf.type = 'text/javascript';
      wf.async = 'true';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(wf, s);
    })();
  </script>



    

<footer class="pa3 h-15 tc absolute bottom-0 w-100">
  <a class="link near-black hover-silver dib h2 w2 mr3" href="https://github.com/smalalur-gh" title="GitHub">
	  <svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M8 0C3.58 0 0 3.582 0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.083-.202-.358-1.015.077-2.117 0 0 .672-.215 2.2.82.638-.178 1.323-.266 2.003-.27.68.004 1.364.092 2.003.27 1.527-1.035 2.198-.82 2.198-.82.437 1.102.163 1.915.08 2.117.513.56.823 1.274.823 2.147 0 3.073-1.87 3.75-3.653 3.947.287.246.543.735.543 1.48 0 1.07-.01 1.933-.01 2.195 0 .215.144.463.55.385C13.71 14.53 16 11.534 16 8c0-4.418-3.582-8-8-8"></path></svg>
  </a>
  <a class="link hover-silver near-black dib h2 w2 mr3" href="https://twitter.com/goopyflux" title="Twitter">
    <svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M16 3.038c-.59.26-1.22.437-1.885.517.677-.407 1.198-1.05 1.443-1.816-.634.375-1.337.648-2.085.795-.598-.638-1.45-1.036-2.396-1.036-1.812 0-3.282 1.468-3.282 3.28 0 .258.03.51.085.75C5.152 5.39 2.733 4.084 1.114 2.1.83 2.583.67 3.147.67 3.75c0 1.14.58 2.143 1.46 2.732-.538-.017-1.045-.165-1.487-.41v.04c0 1.59 1.13 2.918 2.633 3.22-.276.074-.566.114-.865.114-.21 0-.416-.02-.617-.058.418 1.304 1.63 2.253 3.067 2.28-1.124.88-2.54 1.404-4.077 1.404-.265 0-.526-.015-.783-.045 1.453.93 3.178 1.474 5.032 1.474 6.038 0 9.34-5 9.34-9.338 0-.143-.004-.284-.01-.425.64-.463 1.198-1.04 1.638-1.7z" fill-rule="nonzero"/></svg>
  </a>
  <small class="mt3 f6 db tc black-90 fw5">Built by Me, standing on the shoulders of the Open Source Software giants</small>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</footer>


    </body>
</html>

