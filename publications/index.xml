<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications on IBTIAM</title>
    <link>https://smalalur-gh.github.io/publications/</link>
    <description>Recent content in Publications on IBTIAM</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2019 21:40:32 -0600</lastBuildDate>
    
	<atom:link href="https://smalalur-gh.github.io/publications/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Multiple Optimal Learning Factors for Feed-forward Neural Nets</title>
      <link>https://smalalur-gh.github.io/publications/molf/</link>
      <pubDate>Thu, 12 Dec 2019 21:40:32 -0600</pubDate>
      
      <guid>https://smalalur-gh.github.io/publications/molf/</guid>
      <description>A batch training algorithm is developed for a fully connected multi-layer perceptron, with a single hidden layer, which uses two-stages per iteration. In the first stage, Newton&#39;s method is used to find a vector of optimal learning factors (OLFs), one for each hidden unit, which is used to update the input weights. Linear equations are solved for output weights in the second stage. Elements of the new method&#39;s Hessian matrix are shown to be weighted sums of elements from the Hessian of the whole network.</description>
    </item>
    
    <item>
      <title>Optimal Input Gains for Feed-forward Neural Networks</title>
      <link>https://smalalur-gh.github.io/publications/oig/</link>
      <pubDate>Thu, 12 Dec 2019 21:36:52 -0600</pubDate>
      
      <guid>https://smalalur-gh.github.io/publications/oig/</guid>
      <description>In this paper, an effective batch training algorithm is developed for feed-forward networks such as the multilayer perceptron. First, the effects of input transforms are reviewed and explained, using the concept of equivalent networks. Next, a non-singular diagonal transform matrix for the inputs is proposed. Use of this transform is equivalent to altering the input gains in the network. Newtonâ€™s method is used to solve for the input gains and an optimal learning factor.</description>
    </item>
    
  </channel>
</rss>